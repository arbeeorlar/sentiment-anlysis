{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description: \n",
    "\n",
    "General:\n",
    "\n",
    "This project aims to practice deep learning in natual language process by building neural networks to provide sentiment analysis using IMDB movie review dataset.\n",
    "\n",
    "Specific:\n",
    "\n",
    "- Processing text.\n",
    "- Using word embeddings or word vectors (GloVe and Word2Vec)\n",
    "- Using various deep learning model architectures (CNN, RNN)\n",
    "- Document understandings, learnings, tricks, and findings\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First, we will import all the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import bcolz\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tnaduc/Documents/DeepLearning/Projects/SentimentAnalysis\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p data/imdb\n",
    "%mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dowload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl',origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "datafile = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary for this dataset can be downloaded form [here](https://s3.amazonaws.com/text-datasets/imdb_word_index.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with the dataset\n",
    "\n",
    "Let's dig deep to the dataset, collect some initial statistics and get to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25000, 25000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of our training dataset\n",
    "[len(x_train), len(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 433, 149, 124, 121]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x_train[0]), len(x_train[1]), len(x_train[2]), len(x_train[3]), len(x_train[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25000 reviews in each train and test sets. There first review in training set has 138 words (or tokens for more precise) in it, the second has 433, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "PROJECT_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = DATA_HOME_DIR+'imdb/imdb_word_index.json'\n",
    "files = open(filename)\n",
    "vocab =json.load(files)\n",
    "vocab_len = len(vocab)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab is a dict object that maps each word in 88484 words to a unique index (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'fawn', 34701], [u'tsukino', 52006], [u'nunnery', 52007], [u'sonja', 16816], [u'vani', 63951], [u'woods', 1408], [u'spiders', 16115], [u'hanging', 2345], [u'woody', 2289], [u'trawling', 52008]]\n"
     ]
    }
   ],
   "source": [
    "vocab_map = []\n",
    "for key in vocab.keys():\n",
    "    lens =vocab_map.append([key, vocab[key]])\n",
    "print vocab_map[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'and', u'a', u'of', u'to', u'is', u'br', u'in', u'it', u'i']\n"
     ]
    }
   ],
   "source": [
    "# Array of word\n",
    "idx_arr = sorted(vocab, key=vocab.get)\n",
    "print idx_arr[:10],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 'u' on the left each word is just to inform that the string is unicode. It is a feature not an error. \n",
    "\n",
    "We need a mechanism to convert from indices to the actual words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v: k for k, v in vocab.iteritems()}\n",
    "print idx2word[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at one review in our training set and use the map from index to word we just built to construct the whole review in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51, 10, 83, 329, 28117, 70618, 62, 10, 13, 620, 8, 31, 1, 403, 450, 4339, 31, 4868, 54, 28, 2, 145, 26, 2260, 41, 2, 1385, 12, 109, 298, 72, 25, 147, 74, 345, 1, 19, 307, 4, 32, 318, 62, 2, 23, 870, 5, 64, 498, 1, 13311, 4, 360, 7, 7, 561, 28117, 34202, 2, 164, 2326, 23955, 25, 368, 4099, 7, 7, 16, 40, 1, 205, 1163, 4, 7808, 2160, 1698, 2343, 1, 6458, 3317, 4, 4868, 2, 1622, 175, 64, 24, 1648, 16, 1338, 4, 1681, 196, 8, 24, 11255, 110, 6120, 2, 1, 179, 184, 87, 4204, 7, 7, 14, 72, 23, 1722, 5, 1, 1847, 8, 11, 450, 72, 23, 1575, 12, 161, 6, 123, 14, 9, 183, 2, 12, 1, 9982, 1491, 67, 650, 260, 453, 40235, 1, 9465, 5, 730, 3, 271, 395, 31, 3, 182, 129, 502, 80, 3, 110, 2543, 1491, 12, 1522, 4868, 166, 1, 2121, 743, 306, 5, 1668, 20, 2, 844, 927, 7, 7, 42, 5, 75, 12, 88, 81, 77, 795, 11, 19, 10, 61, 132, 12, 85, 1, 853, 295, 77, 239, 101, 2160, 1698, 8, 3, 619, 214, 12, 158, 154, 156, 588, 199, 11, 17, 3, 577, 2160, 1698, 2439, 1, 2598, 72, 29, 212, 166, 2, 137, 140, 8, 3144, 5, 27, 125, 81, 37, 24, 17, 28, 531, 4880, 26, 44, 9648, 53, 14, 32, 281, 2, 90, 157, 486, 415, 4, 495, 7, 7, 446, 2, 156, 10, 856, 10, 261, 3468, 18515, 14, 6120, 2373, 172, 133, 26, 6, 8, 26, 44, 1, 9634, 968, 129, 269, 2, 265, 1366, 42, 11, 11648, 649, 26, 97, 1668, 24, 202, 17, 205, 147, 7, 7, 587'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"when i first read armistead maupins story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the gloss of hollywood br br writer armistead maupin and director patrick stettner have truly succeeded br br with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with issues of trust both in his personnel life jess and the world around him donna br br as we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably the request to review a book written by a young man turns into a life changing event that helps gabriel find the strength within himself to carry on and move forward br br it's to bad that most people will avoid this film i only say that because the average american will probably think robin williams in a serious role that didn't work before please give this movie a chance robin williams touches the darkness we all must find and go through in ourselves to be better people like his movie one hour photo he has stepped up as an actor and made another quality piece of art br br oh and before i forget i believe bobby cannavale as jess steals every scene he is in he has the 1940's leading man looks and screen presence it's this hacks opinion he could carry his own movie right now br br s\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(idx2word[i] for i in x_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some statistics about the lengths of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, x_train))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest review has 2493 words, the shortest has 10 words. On average, there are about 138 words in each review.\n",
    "\n",
    "We need to take a look at the label set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(labels_train)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12500 positive and 12500 negative reviews. They are encoded as 1 and 0, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We need to do some preprocessing before we can build machine learning model to predict sentiments. There are several reasons:\n",
    "\n",
    "- In any writing, there are some common words that will appears more than some rare words. Rare words are not likely affect to much to the semantic of the text. To save the processing time, we will reduce the size of our vocab to some fixed value vocab_size. Since the indices in vocab dictionary are already ordered according the prequencies of appearacne in the reviews for all the words, resizing vocab can be easily done by setting the indices larger than vocab_size to be vocab_size.\n",
    "- The lengths of reviews are different. We need somehow to standardize the length of input sequences. This can be done but choosing a fixed length input_len - a hyperparameter. Then, for the reviews are shorter than input_len we just do zero-padding in front of that review. Whereas, for review longer than input_len, we truncate them to input_len.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab_size = 5000\n",
    "#train  = [np.array([i if i < vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "#test  = [np.array([i if i < vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]\n",
    "\n",
    "train = [np.array([i for i in s]) for s in x_train]\n",
    "test = [np.array([i for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   51    10    83   329 28117 70618    62    10    13   620     8    31\n",
      "     1   403   450  4339    31  4868    54    28     2   145    26  2260\n",
      "    41     2  1385    12   109   298    72    25   147    74   345     1\n",
      "    19   307     4    32   318    62     2    23   870     5    64   498\n",
      "     1 13311     4   360     7     7   561 28117 34202     2   164  2326\n",
      " 23955    25   368  4099     7     7    16    40     1   205  1163     4\n",
      "  7808  2160  1698  2343     1  6458  3317     4  4868     2  1622   175\n",
      "    64    24  1648    16  1338     4  1681   196     8    24 11255   110\n",
      "  6120     2     1   179   184    87  4204     7     7    14    72    23\n",
      "  1722     5     1  1847     8    11   450    72    23  1575    12   161\n",
      "     6   123    14     9   183     2    12     1  9982  1491    67   650\n",
      "   260   453 40235     1  9465     5   730     3   271   395    31     3\n",
      "   182   129   502    80     3   110  2543  1491    12  1522  4868   166\n",
      "     1  2121   743   306     5  1668    20     2   844   927     7     7\n",
      "    42     5    75    12    88    81    77   795    11    19    10    61\n",
      "   132    12    85     1   853   295    77   239   101  2160  1698     8\n",
      "     3   619   214    12   158   154   156   588   199    11    17     3\n",
      "   577  2160  1698  2439     1  2598    72    29   212   166     2   137\n",
      "   140     8  3144     5    27   125    81    37    24    17    28   531\n",
      "  4880    26    44  9648    53    14    32   281     2    90   157   486\n",
      "   415     4   495     7     7   446     2   156    10   856    10   261\n",
      "  3468 18515    14  6120  2373   172   133    26     6     8    26    44\n",
      "     1  9634   968   129   269     2   265  1366    42    11 11648   649\n",
      "    26    97  1668    24   202    17   205   147     7     7   587]\n"
     ]
    }
   ],
   "source": [
    "print train[10],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose input length of 500, about twice as big as the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_len =500\n",
    "train = sequence.pad_sequences(train, maxlen=input_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=input_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0    51    10    83\n",
      "   329 28117 70618    62    10    13   620     8    31     1   403   450\n",
      "  4339    31  4868    54    28     2   145    26  2260    41     2  1385\n",
      "    12   109   298    72    25   147    74   345     1    19   307     4\n",
      "    32   318    62     2    23   870     5    64   498     1 13311     4\n",
      "   360     7     7   561 28117 34202     2   164  2326 23955    25   368\n",
      "  4099     7     7    16    40     1   205  1163     4  7808  2160  1698\n",
      "  2343     1  6458  3317     4  4868     2  1622   175    64    24  1648\n",
      "    16  1338     4  1681   196     8    24 11255   110  6120     2     1\n",
      "   179   184    87  4204     7     7    14    72    23  1722     5     1\n",
      "  1847     8    11   450    72    23  1575    12   161     6   123    14\n",
      "     9   183     2    12     1  9982  1491    67   650   260   453 40235\n",
      "     1  9465     5   730     3   271   395    31     3   182   129   502\n",
      "    80     3   110  2543  1491    12  1522  4868   166     1  2121   743\n",
      "   306     5  1668    20     2   844   927     7     7    42     5    75\n",
      "    12    88    81    77   795    11    19    10    61   132    12    85\n",
      "     1   853   295    77   239   101  2160  1698     8     3   619   214\n",
      "    12   158   154   156   588   199    11    17     3   577  2160  1698\n",
      "  2439     1  2598    72    29   212   166     2   137   140     8  3144\n",
      "     5    27   125    81    37    24    17    28   531  4880    26    44\n",
      "  9648    53    14    32   281     2    90   157   486   415     4   495\n",
      "     7     7   446     2   156    10   856    10   261  3468 18515    14\n",
      "  6120  2373   172   133    26     6     8    26    44     1  9634   968\n",
      "   129   269     2   265  1366    42    11 11648   649    26    97  1668\n",
      "    24   202    17   205   147     7     7   587]\n"
     ]
    }
   ],
   "source": [
    "print train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at our input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approach\n",
    "\n",
    "Now that we already processed our data nice and clean, we are ready to build deep learning models. Before we build anything, probably the first idea is to ask *do we need to build our machine learning systems from scratch?* In most cases, the answer is NO, we can build our model based upon something (model architectures, model weights) which is already proven to work well and \"intuitively\" adapt to our specific problem.\n",
    "\n",
    "### Standing on the giants' shoulders\n",
    "\n",
    "Just like what we see in using convolutional networks with well-known architectures such as vgg or resnet and used their respective ImageNet pre-trained weights, we can leverage a lot of benefits such as our classification categories may well be contained in ImageNet 1000 categories, the networks already learned so many useful feaures, etc. Similarly, for NLP tasks like this we can ask is there anything similar to pretraind ImageNet weights? The answer is yes. If you think about it, pretrained words may be even clearer than pretrained images, why? because the word \"deep learning\" in any documents or test sets will be the same, it is \"deep learning\". Whereas pre-trained images of dogs may very different from the dogs in test images.\n",
    "\n",
    "By far the most popular \"pre-trained weights\" for words are Word2Vec from Google and GloVe from Stanford NLP lab. These are technically called word embeddings or word vector representations.\n",
    "\n",
    "### Represent stuff by numerical vectors\n",
    "\n",
    "Okie, but what is word embedding by the way?\n",
    "\n",
    "*** INSIGHT***\n",
    "\n",
    "Machine learning models work with numbers. Therefore, in order to input data (text, images, audio) to machine learning models we need somehow *convert* each of the data to a numeric value. Image and audio data are easy because they are intrinsically numerical in the way they are represented. How is about word?\n",
    "\n",
    "It is a good (primative) way to represent (or index) each word as an integer because it is easy to look up and manipulate. Fromt here, there are other methods to represent words: onehot encoding and word embedding (or word vectors):\n",
    "\n",
    "\n",
    "1. **Onehot encoding** explodes the whole vocabulary to \"onehot\" vectors. Each word is assigned to an vector which is of the length of vocabulary size. An onehot vector is zero everywhere except for where the entry is equal to the index of that word in the vocabulary. Onehot encoding is simple, intuitive and has many disadvantages:\n",
    "\n",
    " First, the size of input scales with the size of the vocab. If the vocab is 100000 words and each input sequence is of 10 words long, then one input is of the size 10x100000. Moreover, eventhough the input data is big, it is sparse, therefore, working with it is computaionally expensive. \n",
    " \n",
    " Secondly, the representation does not encode much useful semantic information. Due to the orthogonal and sparse representation, distance between any 2 word (norm-2) is exactly $\\sqrt2$. That means relationship between words \"dog\" and \"puppy\" is no difference from \"dog\" and \"engine\". \n",
    " \n",
    " How can we improve onehot encoding? one idea is reduce the dimension of the representation by SVD. There are a lot of work done in this direction like latent semantic analysis but it also has its own problems such as computational expensive (matrix factorization), issue with new words (have to run SVD every time a new word is introduced to the vocab) etc.\n",
    " \n",
    " Another direction is directly model every word by a dense and fixed length vector. That leads to the concept of word embeddings.\n",
    "\n",
    "\n",
    "- **Word embedding** maps each word with a numerical vector of prefixed length putting together to form a matrix. How to obtain these vectors? There are different ways, e.g., language models, to do this but the central idea is basically to characterize a word by the words around it (literally left and right).\n",
    "\n",
    "  \" You shall know a word by the company it keeps\" - J.R. Firth (1957)\n",
    " \n",
    " So, people (who trained word embeddings such as word2vec of Google or GloVe of Stanford NLP lab) trained their language model on large corpa such as wikipedia dump or entire internet dump. \n",
    " \n",
    " Specifically, for example, in Google word2vec's continuos bag of words model, the probability of any word in the corpus is conditioned to probabilities of, say, 10 words to the left and 10 words to the right by using the softmax function. Then the model is trained by maximizing the log-likehood using gradient descent for all the words in the entire corpus. There are some technical details on how to obtain the conditional probability by using negative sampling to avoid calculating the expensive total probablities. Nevertheless, the math is relatively simple.\n",
    " \n",
    " This training resutls in a vector representations of words that is able to capture semantic information such as in terms of their l2-distance: \"king\"-\"queen\" = \"man\"-\"woman\" or words like \"president\", \"minister\", \"governor\", \"senator\" etc are clustered closely to each other (politic cluster). Surprised? Magic? Not quite, if you think you about it. This is expected because these words normally appears in the same contexts, maybe same sentences. So natually as we maximize the probablity that they appear together we should certainly get these resutls.\n",
    "\n",
    "\n",
    "We will choose GloVe embedding for this particular problem because it is a litle bit faster than Google's one. It would be nice to use other word embeddings as well, to have a comparison, but we stick to GloVe for the moment. \n",
    "\n",
    "- We use GloVe 6 billion tokens downloaded from [here](https://nlp.stanford.edu/projects/glove/).\n",
    "- This version has 4 embeddings with different dimensions: 50, 100, 200, 300. First try we will use 50 as the dimension of our embedding. Then we can use others to compare.\n",
    "\n",
    "# 3. Preprocessing pretrained word embedding.\n",
    "\n",
    "After downloading the 6B token version from the link above, let's open the 50 dimensional embedding and split very line and look at the first line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(DATA_HOME_DIR +'glove.6B/glove.6B.50d.txt') as f: lines = [line.split() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe vocabulary size\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457', '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843', '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658', '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801', '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823', '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071', '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449', '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871', '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785', '0.0027849', '-0.18411', '-0.11514', '-0.78581']\n"
     ]
    }
   ],
   "source": [
    "print lines[0],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "so, the first element is the word 'the' and following it is 50 numerical values that represent that word.\n",
    "\n",
    "Now, let take the first element of every line and put into a list of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_vocab = [line[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n"
     ]
    }
   ],
   "source": [
    "# see first 20 words\n",
    "print glove_vocab[:20],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's index these words for quick lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordidx = {o:i for i,o in enumerate(glove_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We also need to separate the numerical parts and put them into an 2darray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_embedding = np.stack(np.array(d[1:], dtype = np.float32) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(DATA_HOME_DIR +'glove.6B/', 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.18000013e-01   2.49679998e-01  -4.12420005e-01   1.21699996e-01\n",
      "    3.45270008e-01  -4.44569997e-02  -4.96879995e-01  -1.78619996e-01\n",
      "   -6.60229998e-04  -6.56599998e-01   2.78430015e-01  -1.47670001e-01\n",
      "   -5.56770027e-01   1.46579996e-01  -9.50950012e-03   1.16579998e-02\n",
      "    1.02040000e-01  -1.27920002e-01  -8.44299972e-01  -1.21809997e-01\n",
      "   -1.68009996e-02  -3.32789987e-01  -1.55200005e-01  -2.31309995e-01\n",
      "   -1.91809997e-01  -1.88230002e+00  -7.67459989e-01   9.90509987e-02\n",
      "   -4.21249986e-01  -1.95260003e-01   4.00710011e+00  -1.85939997e-01\n",
      "   -5.22870004e-01  -3.16810012e-01   5.92130003e-04   7.44489999e-03\n",
      "    1.77780002e-01  -1.58969998e-01   1.20409997e-02  -5.42230010e-02\n",
      "   -2.98709989e-01  -1.57490000e-01  -3.47579986e-01  -4.56370004e-02\n",
      "   -4.42510009e-01   1.87849998e-01   2.78489990e-03  -1.84110001e-01\n",
      "   -1.15139998e-01  -7.85809994e-01]\n",
      " [  1.34410001e-02   2.36819997e-01  -1.68990001e-01   4.09509987e-01\n",
      "    6.38119996e-01   4.77090001e-01  -4.28519994e-01  -5.56410015e-01\n",
      "   -3.63999993e-01  -2.39380002e-01   1.30009994e-01  -6.37340024e-02\n",
      "   -3.95749986e-01  -4.81620014e-01   2.32910007e-01   9.02009979e-02\n",
      "   -1.33239999e-01   7.86390007e-02  -4.16339993e-01  -1.54280007e-01\n",
      "    1.00680001e-01   4.88909990e-01   3.12260002e-01  -1.25200003e-01\n",
      "   -3.75120007e-02  -1.51789999e+00   1.26120001e-01  -2.44200006e-02\n",
      "   -4.29610014e-02  -2.83510000e-01   3.54159999e+00  -1.19560003e-01\n",
      "   -1.45330001e-02  -1.49900004e-01   2.18640000e-01  -3.34120005e-01\n",
      "   -1.38720006e-01   3.18060011e-01   7.03580022e-01   4.48579997e-01\n",
      "   -8.02619979e-02   6.30029976e-01   3.21110010e-01  -4.67649996e-01\n",
      "    2.27860004e-01   3.60339999e-01  -3.78179997e-01  -5.66569984e-01\n",
      "    4.46910001e-02   3.03920001e-01]]\n"
     ]
    }
   ],
   "source": [
    "print w_embedding[:2,:],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save these preprocessing data so that we do not have to run this again next time.\n",
    "\n",
    "### Save preprocessed GloVe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(glove_vocab, open(DATA_HOME_DIR+'glove.6B/glove_vocab_50.pkl','wb'))\n",
    "pickle.dump(wordidx, open(DATA_HOME_DIR+'glove.6B/word_idx_50.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data(path, array):\n",
    "    data=bcolz.carray(array, rootdir=path, mode='w')\n",
    "    data.flush()\n",
    "save_data(DATA_HOME_DIR+'glove.6B/w_embedding.dat', w_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load  preprocessed GloVe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_embedding = bcolz.open(DATA_HOME_DIR+'glove.6B/w_embedding.dat')[:]\n",
    "glove_vocab= pickle.load(open(DATA_HOME_DIR+'glove.6B/glove_vocab_50.pkl','rb'))\n",
    "glove_idx= pickle.load(open(DATA_HOME_DIR+'glove.6B/word_idx_50.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still small problems. First, the indices in GloVe vocab is different from our IMDB vocab. Second, there might be some words that apprear in our vocab but do not appear in GloVe vocab as the consequence of different tokenization methods. Third, as we set our rare words to have the same index (4999), how should we set weights for them. For the second and third problems, we just randomly initialize those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need regular expression package to look up word in GloVe data\n",
    "import re\n",
    "\n",
    "embed =  np.zeros((vocab_size, 50))\n",
    "\n",
    "for i in range(1,vocab_size):\n",
    "    word = idx2word[i]\n",
    "    if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "        g_idx = glove_idx[word]\n",
    "        embed[i] = w_embedding[g_idx]\n",
    "    else :\n",
    "        embed[i] = np.random.uniform(-1, 1, size=(50,))\n",
    "embed[-1] = np.random.uniform(-1, 1, size=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_matrix = np.zeros((vocab_len, 50))\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embed_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "          1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "         -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "         -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "         -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "          1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "         -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "         -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "         -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "          9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "          4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "         -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "          1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "         -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "         -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "          1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "         -1.15139998e-01,  -7.85809994e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix[:2,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'the'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the', u'and', u'a', u'of', u'to', u'is', u'br', u'in', u'it', u'i']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_embedding[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'the'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emb = create_emb()\n",
    "idx2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nnnnnnnhe'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =4\n",
    "if idx2word[2] and re.match(r\"^[a-zA-Z0-9\\-]*$\",'the'):\n",
    "    a =5\n",
    "\n",
    "a\n",
    "b= idx2word[2] and re.match(r\"^[a-zA-Z0-9\\-]*$\",'nnnnnnnhe')\n",
    "\n",
    "b.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "          1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "         -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "         -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "         -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "          1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "         -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "         -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "         -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "          9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "          4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "         -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "          1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "         -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "         -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "          1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "         -1.15139998e-01,  -7.85809994e-01],\n",
       "       [  2.68180013e-01,   1.43460006e-01,  -2.78770000e-01,\n",
       "          1.62569992e-02,   1.13839999e-01,   6.99230015e-01,\n",
       "         -5.13320029e-01,  -4.73679990e-01,  -3.30749989e-01,\n",
       "         -1.38339996e-01,   2.70200014e-01,   3.09379995e-01,\n",
       "         -4.50120002e-01,  -4.12699997e-01,  -9.93200019e-02,\n",
       "          3.80849987e-02,   2.97490004e-02,   1.00759998e-01,\n",
       "         -2.50580013e-01,  -5.18180013e-01,   3.45580012e-01,\n",
       "          4.49220002e-01,   4.87910002e-01,  -8.08660015e-02,\n",
       "         -1.01209998e-01,  -1.37769997e+00,  -1.08659998e-01,\n",
       "         -2.32010007e-01,   1.28389997e-02,  -4.65079993e-01,\n",
       "          3.84629989e+00,   3.13620001e-01,   1.36429995e-01,\n",
       "         -5.22440016e-01,   3.30199987e-01,   3.37069988e-01,\n",
       "         -3.56009990e-01,   3.24310005e-01,   1.20410003e-01,\n",
       "          3.51200014e-01,  -6.90430030e-02,   3.68849993e-01,\n",
       "          2.51679987e-01,  -2.45169997e-01,   2.53809988e-01,\n",
       "          1.36700004e-01,  -3.11780006e-01,  -6.32099986e-01,\n",
       "         -2.50279993e-01,  -3.80970001e-01],\n",
       "       [  2.17050001e-01,   4.65149999e-01,  -4.67570007e-01,\n",
       "          1.00819997e-01,   1.01349998e+00,   7.48449981e-01,\n",
       "         -5.31040013e-01,  -2.62560010e-01,   1.68119997e-01,\n",
       "          1.31819993e-01,  -2.49090001e-01,  -4.41850007e-01,\n",
       "         -2.17390001e-01,   5.10039985e-01,   1.34480000e-01,\n",
       "         -4.31410015e-01,  -3.12300008e-02,   2.06740007e-01,\n",
       "         -7.81379998e-01,  -2.01480001e-01,  -9.74010006e-02,\n",
       "          1.60879999e-01,  -6.18359983e-01,  -1.85039997e-01,\n",
       "         -1.24609999e-01,  -2.25259995e+00,  -2.23210007e-01,\n",
       "          5.04299998e-01,   3.22569996e-01,   1.53129995e-01,\n",
       "          3.96359992e+00,  -7.13649988e-01,  -6.70120001e-01,\n",
       "          2.83879995e-01,   2.17380002e-01,   1.44329995e-01,\n",
       "          2.59259999e-01,   2.34339997e-01,   4.27399993e-01,\n",
       "         -4.44510013e-01,   1.38129994e-01,   3.69729996e-01,\n",
       "         -6.42889977e-01,   2.41420008e-02,  -3.93150002e-02,\n",
       "         -2.60369986e-01,   1.20169997e-01,  -4.37819995e-02,\n",
       "          4.10129994e-01,   1.79600000e-01],\n",
       "       [  7.08530009e-01,   5.70879996e-01,  -4.71599996e-01,\n",
       "          1.80480003e-01,   5.44489980e-01,   7.26029992e-01,\n",
       "          1.81569993e-01,  -5.23930013e-01,   1.03809997e-01,\n",
       "         -1.75659999e-01,   7.88519979e-02,  -3.62159997e-01,\n",
       "         -1.18290000e-01,  -8.33360016e-01,   1.19170003e-01,\n",
       "         -1.66050002e-01,   6.15549982e-02,  -1.27189998e-02,\n",
       "         -5.66229999e-01,   1.36160003e-02,   2.28510007e-01,\n",
       "         -1.43959999e-01,  -6.75489977e-02,  -3.81570011e-01,\n",
       "         -2.36980006e-01,  -1.70369995e+00,  -8.66919994e-01,\n",
       "         -2.67040014e-01,  -2.58899987e-01,   1.76699996e-01,\n",
       "          3.86759996e+00,  -1.61300004e-01,  -1.32730007e-01,\n",
       "         -6.88809991e-01,   1.84440002e-01,   5.24639990e-03,\n",
       "         -3.38739991e-01,  -7.89560005e-02,   2.41850004e-01,\n",
       "          3.65759999e-01,  -3.47270012e-01,   2.84830004e-01,\n",
       "          7.56929964e-02,  -6.21780008e-02,  -3.89880002e-01,\n",
       "          2.29020000e-01,  -2.16169998e-01,  -2.25620002e-01,\n",
       "         -9.39180031e-02,  -8.03749979e-01],\n",
       "       [  6.80469990e-01,  -3.92629988e-02,   3.01860005e-01,\n",
       "         -1.77919999e-01,   4.29619998e-01,   3.22460011e-02,\n",
       "         -4.13760006e-01,   1.32280007e-01,  -2.98469990e-01,\n",
       "         -8.52530003e-02,   1.71179995e-01,   2.24189997e-01,\n",
       "         -1.00460000e-01,  -4.36529994e-01,   3.34179997e-01,\n",
       "          6.78460002e-01,   5.72040007e-02,  -3.44480008e-01,\n",
       "         -4.27850008e-01,  -4.32749987e-01,   5.59629977e-01,\n",
       "          1.00319996e-01,   1.86770007e-01,  -2.68539995e-01,\n",
       "          3.73339988e-02,  -2.09319997e+00,   2.21709996e-01,\n",
       "         -3.98680001e-01,   2.09120005e-01,  -5.57250023e-01,\n",
       "          3.88260007e+00,   4.74660009e-01,  -9.56579983e-01,\n",
       "         -3.77880007e-01,   2.08690003e-01,  -3.27520013e-01,\n",
       "          1.27509996e-01,   8.83589983e-02,   1.63509995e-01,\n",
       "         -2.16340005e-01,  -9.43749994e-02,   1.83240008e-02,\n",
       "          2.10480005e-01,  -3.08800004e-02,  -1.97219998e-01,\n",
       "          8.22789967e-02,  -9.43399966e-02,  -7.32970014e-02,\n",
       "         -6.46990016e-02,  -2.60439992e-01],\n",
       "       [  6.18499994e-01,   6.42539978e-01,  -4.65519994e-01,\n",
       "          3.75699997e-01,   7.48380005e-01,   5.37389994e-01,\n",
       "          2.22390005e-03,  -6.05769992e-01,   2.64079988e-01,\n",
       "          1.17030002e-01,   4.37220007e-01,   2.00920001e-01,\n",
       "         -5.78589998e-02,  -3.45889986e-01,   2.16639996e-01,\n",
       "          5.85730016e-01,   5.39189994e-01,   6.94899976e-01,\n",
       "         -1.56179994e-01,   5.58300018e-02,  -6.05149984e-01,\n",
       "         -2.89970011e-01,  -2.55939998e-02,   5.55930018e-01,\n",
       "          2.53560007e-01,  -1.96120000e+00,  -5.13809979e-01,\n",
       "          6.90959990e-01,   6.62460029e-02,  -5.42239994e-02,\n",
       "          3.78710008e+00,  -7.74030030e-01,  -1.26890004e-01,\n",
       "         -5.14649987e-01,   6.67050034e-02,  -3.29329997e-01,\n",
       "          1.34829998e-01,   1.90490007e-01,   1.38119996e-01,\n",
       "         -2.15030000e-01,  -1.65730007e-02,   3.12000006e-01,\n",
       "         -3.31889987e-01,  -2.60010008e-02,  -3.82030010e-01,\n",
       "          1.94030002e-01,  -1.24660000e-01,  -2.75570005e-01,\n",
       "          3.08990002e-01,   4.84970003e-01],\n",
       "       [ -1.71680003e-01,   3.85500014e-01,   5.01950026e-01,\n",
       "         -5.89349985e-01,  -1.05040002e+00,   3.13270003e-01,\n",
       "          1.24629997e-01,  -1.36000001e+00,  -1.25059998e+00,\n",
       "         -9.46560025e-01,   2.86440015e-01,   1.25320005e+00,\n",
       "         -5.41249990e-01,  -2.54940003e-01,  -6.96349978e-01,\n",
       "          1.53659999e-01,  -9.22800004e-01,   7.08419979e-01,\n",
       "         -3.87080014e-01,   4.55159992e-01,  -2.61579990e-01,\n",
       "         -8.29180002e-01,   4.52840000e-01,   4.60740000e-01,\n",
       "         -6.06289983e-01,   3.18250000e-01,  -5.66070020e-01,\n",
       "          1.92540005e-01,  -2.22890005e-01,   1.51749998e-01,\n",
       "          3.76150012e-01,   1.51869997e-01,   7.84250021e-01,\n",
       "          1.17229998e+00,   4.41890001e-01,  -1.75579995e-01,\n",
       "          8.35280001e-01,  -7.69230008e-01,  -1.99970007e-01,\n",
       "          5.03549993e-01,   7.05609977e-01,  -3.19220006e-01,\n",
       "          2.07739994e-01,   3.01789995e-02,  -3.16080004e-01,\n",
       "         -2.08859995e-01,   2.01440006e-02,  -1.93140000e-01,\n",
       "          6.01819992e-01,   7.14730024e-01],\n",
       "       [  3.30419987e-01,   2.49950007e-01,  -6.08739972e-01,\n",
       "          1.09229997e-01,   3.63719985e-02,   1.50999993e-01,\n",
       "         -5.50830007e-01,  -7.42390007e-02,  -9.23070014e-02,\n",
       "         -3.28209996e-01,   9.59800035e-02,  -8.22690010e-01,\n",
       "         -3.67170006e-01,  -6.70090020e-01,   4.29089993e-01,\n",
       "          1.64960008e-02,  -2.35730007e-01,   1.28639996e-01,\n",
       "         -1.09529996e+00,   4.33340013e-01,   5.70670009e-01,\n",
       "         -1.03600003e-01,   2.04219997e-01,   7.83080012e-02,\n",
       "         -4.27949995e-01,  -1.79840004e+00,  -2.78649986e-01,\n",
       "          1.19539998e-01,  -1.26890004e-01,   3.17439996e-02,\n",
       "          3.86310005e+00,  -1.77860007e-01,  -8.24339986e-02,\n",
       "         -6.26980007e-01,   2.64970005e-01,  -5.71850017e-02,\n",
       "         -7.35210031e-02,   4.61030006e-01,   3.08620006e-01,\n",
       "          1.24980003e-01,  -4.86090004e-01,  -8.02719966e-03,\n",
       "          3.11840009e-02,  -3.65759999e-01,  -4.26990002e-01,\n",
       "          4.21640009e-01,  -1.16659999e-01,  -5.07030010e-01,\n",
       "         -2.72729993e-02,  -5.32850027e-01],\n",
       "       [  6.11829996e-01,  -2.20719993e-01,  -1.08980000e-01,\n",
       "         -5.29670008e-02,   5.08040011e-01,   3.46839994e-01,\n",
       "         -3.35579991e-01,  -1.91520005e-01,  -3.58650014e-02,\n",
       "          1.05099998e-01,   7.93500021e-02,   2.44900003e-01,\n",
       "         -4.37299997e-01,  -3.33440006e-01,   5.74790001e-01,\n",
       "          6.90519989e-01,   2.97129989e-01,   9.06689987e-02,\n",
       "         -5.49920022e-01,  -4.61760014e-01,   1.01130001e-01,\n",
       "         -2.02399995e-02,   2.84790009e-01,   4.35120016e-02,\n",
       "          4.57349986e-01,  -2.04660010e+00,  -5.80839992e-01,\n",
       "          6.17969990e-01,   6.51799977e-01,  -5.82629979e-01,\n",
       "          4.07859993e+00,  -2.54200011e-01,  -1.46489993e-01,\n",
       "         -3.43210012e-01,  -2.54370004e-01,  -4.46770012e-01,\n",
       "          1.26570001e-01,   2.81340003e-01,   1.33310005e-01,\n",
       "         -3.69740009e-01,   5.00589982e-02,  -1.00579999e-01,\n",
       "         -1.79069992e-02,   1.11419998e-01,  -7.17980027e-01,\n",
       "          4.90999997e-01,  -9.99739990e-02,  -4.36879992e-02,\n",
       "         -9.79219973e-02,   1.68060005e-01]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish preprocessing GloVe word embedding data.\n",
    "\n",
    "# 4. Building Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
