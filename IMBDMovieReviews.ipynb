{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description: \n",
    "\n",
    "General:\n",
    "\n",
    "This project aims to practice deep learning in natual language process by building neural networks to provide sentiment analysis using IMDB movie review dataset.\n",
    "\n",
    "Specific:\n",
    "\n",
    "- Processing text.\n",
    "- Using word embeddings or word vectors (GloVe and Word2Vec)\n",
    "- Using various deep learning model architectures (CNN, RNN)\n",
    "- Document understandings, learnings, tricks, and findings\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First, we will import all the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import sequence\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tnaduc/Documents/DeepLearning/Projects/Sentiment Analysis\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p data/imdb\n",
    "%mkdir models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dowload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl',origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "datafile = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary for this dataset can be downloaded form [here](https://s3.amazonaws.com/text-datasets/imdb_word_index.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with the dataset\n",
    "\n",
    "Let's dig deep to the dataset, collect some initial statistics and get to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25000, 25000]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of our training dataset\n",
    "[len(x_train), len(x_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[138, 433, 149, 124, 121]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x_train[0]), len(x_train[1]), len(x_train[2]), len(x_train[3]), len(x_train[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 25000 reviews in each train and test sets. There first review in training set has 138 words (or tokens for more precise) in it, the second has 433, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "PROJECT_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = DATA_HOME_DIR+'imdb/imdb_word_index.json'\n",
    "files = open(filename)\n",
    "vocab =json.load(files)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab is a dict object that maps each word in 88484 words to a unique index (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'fawn', 34701], [u'tsukino', 52006], [u'nunnery', 52007], [u'sonja', 16816], [u'vani', 63951], [u'woods', 1408], [u'spiders', 16115], [u'hanging', 2345], [u'woody', 2289], [u'trawling', 52008]]\n"
     ]
    }
   ],
   "source": [
    "vocab_map = []\n",
    "for key in vocab.keys():\n",
    "    lens =vocab_map.append([key, vocab[key]])\n",
    "print vocab_map[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'and', u'a', u'of', u'to', u'is', u'br', u'in', u'it', u'i']\n"
     ]
    }
   ],
   "source": [
    "# Array of word\n",
    "idx_arr = sorted(vocab, key=vocab.get)\n",
    "print idx_arr[:10],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the 'u' on the left each word is just to inform that the string is unicode. It is a feature not an error. \n",
    "\n",
    "We need a mechanism to convert from indices to the actual words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v: k for k, v in vocab.iteritems()}\n",
    "print idx2word[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at one review in our training set and use the map from index to word we just built to construct the whole review in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51, 10, 83, 329, 28117, 70618, 62, 10, 13, 620, 8, 31, 1, 403, 450, 4339, 31, 4868, 54, 28, 2, 145, 26, 2260, 41, 2, 1385, 12, 109, 298, 72, 25, 147, 74, 345, 1, 19, 307, 4, 32, 318, 62, 2, 23, 870, 5, 64, 498, 1, 13311, 4, 360, 7, 7, 561, 28117, 34202, 2, 164, 2326, 23955, 25, 368, 4099, 7, 7, 16, 40, 1, 205, 1163, 4, 7808, 2160, 1698, 2343, 1, 6458, 3317, 4, 4868, 2, 1622, 175, 64, 24, 1648, 16, 1338, 4, 1681, 196, 8, 24, 11255, 110, 6120, 2, 1, 179, 184, 87, 4204, 7, 7, 14, 72, 23, 1722, 5, 1, 1847, 8, 11, 450, 72, 23, 1575, 12, 161, 6, 123, 14, 9, 183, 2, 12, 1, 9982, 1491, 67, 650, 260, 453, 40235, 1, 9465, 5, 730, 3, 271, 395, 31, 3, 182, 129, 502, 80, 3, 110, 2543, 1491, 12, 1522, 4868, 166, 1, 2121, 743, 306, 5, 1668, 20, 2, 844, 927, 7, 7, 42, 5, 75, 12, 88, 81, 77, 795, 11, 19, 10, 61, 132, 12, 85, 1, 853, 295, 77, 239, 101, 2160, 1698, 8, 3, 619, 214, 12, 158, 154, 156, 588, 199, 11, 17, 3, 577, 2160, 1698, 2439, 1, 2598, 72, 29, 212, 166, 2, 137, 140, 8, 3144, 5, 27, 125, 81, 37, 24, 17, 28, 531, 4880, 26, 44, 9648, 53, 14, 32, 281, 2, 90, 157, 486, 415, 4, 495, 7, 7, 446, 2, 156, 10, 856, 10, 261, 3468, 18515, 14, 6120, 2373, 172, 133, 26, 6, 8, 26, 44, 1, 9634, 968, 129, 269, 2, 265, 1366, 42, 11, 11648, 649, 26, 97, 1668, 24, 202, 17, 205, 147, 7, 7, 587'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"when i first read armistead maupins story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the gloss of hollywood br br writer armistead maupin and director patrick stettner have truly succeeded br br with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with issues of trust both in his personnel life jess and the world around him donna br br as we are introduced to the players in this drama we are reminded that nothing is ever as it seems and that the smallest event can change our lives irrevocably the request to review a book written by a young man turns into a life changing event that helps gabriel find the strength within himself to carry on and move forward br br it's to bad that most people will avoid this film i only say that because the average american will probably think robin williams in a serious role that didn't work before please give this movie a chance robin williams touches the darkness we all must find and go through in ourselves to be better people like his movie one hour photo he has stepped up as an actor and made another quality piece of art br br oh and before i forget i believe bobby cannavale as jess steals every scene he is in he has the 1940's leading man looks and screen presence it's this hacks opinion he could carry his own movie right now br br s\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(idx2word[i] for i in x_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some statistics about the lengths of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(map(len, x_train))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest review has 2493 words, the shortest has 10 words. On average, there are about 138 words in each review.\n",
    "\n",
    "We need to take a look at the label set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 12500, 1: 12500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(labels_train)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12500 positive and 12500 negative reviews. They are encoded as 1 and 0, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We need to do some preprocessing before we can build machine learning model to predict sentiments. There are several reasons:\n",
    "\n",
    "- In any writing, there are some common words that will appears more than some rare words. Rare words are not likely affect to much to the semantic of the text. To save the processing time, we will reduce the size of our vocab to some fixed value vocab_size. Since the indices in vocab dictionary are already ordered according the prequencies of appearacne in the reviews for all the words, resizing vocab can be easily done by setting the indices larger than vocab_size to be vocab_size.\n",
    "- The lengths of reviews are different. We need somehow to standardize the length of input sequences. This can be done but choosing a fixed length input_len - a hyperparameter. Then, for the reviews are shorter than input_len we just do zero-padding in front of that review. Whereas, for review longer than input_len, we truncate them to input_len.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "train  = [np.array([i if i < vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test  = [np.array([i if i < vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  51   10   83  329 4999 4999   62   10   13  620    8   31    1  403  450\n",
      " 4339   31 4868   54   28    2  145   26 2260   41    2 1385   12  109  298\n",
      "   72   25  147   74  345    1   19  307    4   32  318   62    2   23  870\n",
      "    5   64  498    1 4999    4  360    7    7  561 4999 4999    2  164 2326\n",
      " 4999   25  368 4099    7    7   16   40    1  205 1163    4 4999 2160 1698\n",
      " 2343    1 4999 3317    4 4868    2 1622  175   64   24 1648   16 1338    4\n",
      " 1681  196    8   24 4999  110 4999    2    1  179  184   87 4204    7    7\n",
      "   14   72   23 1722    5    1 1847    8   11  450   72   23 1575   12  161\n",
      "    6  123   14    9  183    2   12    1 4999 1491   67  650  260  453 4999\n",
      "    1 4999    5  730    3  271  395   31    3  182  129  502   80    3  110\n",
      " 2543 1491   12 1522 4868  166    1 2121  743  306    5 1668   20    2  844\n",
      "  927    7    7   42    5   75   12   88   81   77  795   11   19   10   61\n",
      "  132   12   85    1  853  295   77  239  101 2160 1698    8    3  619  214\n",
      "   12  158  154  156  588  199   11   17    3  577 2160 1698 2439    1 2598\n",
      "   72   29  212  166    2  137  140    8 3144    5   27  125   81   37   24\n",
      "   17   28  531 4880   26   44 4999   53   14   32  281    2   90  157  486\n",
      "  415    4  495    7    7  446    2  156   10  856   10  261 3468 4999   14\n",
      " 4999 2373  172  133   26    6    8   26   44    1 4999  968  129  269    2\n",
      "  265 1366   42   11 4999  649   26   97 1668   24  202   17  205  147    7\n",
      "    7  587]\n"
     ]
    }
   ],
   "source": [
    "print train[10],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose input length of 500, about twice as big as the average length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_len =500\n",
    "train = sequence.pad_sequences(train, maxlen=input_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=input_len, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   51   10   83  329 4999 4999   62   10   13  620    8   31\n",
      "    1  403  450 4339   31 4868   54   28    2  145   26 2260   41    2 1385\n",
      "   12  109  298   72   25  147   74  345    1   19  307    4   32  318   62\n",
      "    2   23  870    5   64  498    1 4999    4  360    7    7  561 4999 4999\n",
      "    2  164 2326 4999   25  368 4099    7    7   16   40    1  205 1163    4\n",
      " 4999 2160 1698 2343    1 4999 3317    4 4868    2 1622  175   64   24 1648\n",
      "   16 1338    4 1681  196    8   24 4999  110 4999    2    1  179  184   87\n",
      " 4204    7    7   14   72   23 1722    5    1 1847    8   11  450   72   23\n",
      " 1575   12  161    6  123   14    9  183    2   12    1 4999 1491   67  650\n",
      "  260  453 4999    1 4999    5  730    3  271  395   31    3  182  129  502\n",
      "   80    3  110 2543 1491   12 1522 4868  166    1 2121  743  306    5 1668\n",
      "   20    2  844  927    7    7   42    5   75   12   88   81   77  795   11\n",
      "   19   10   61  132   12   85    1  853  295   77  239  101 2160 1698    8\n",
      "    3  619  214   12  158  154  156  588  199   11   17    3  577 2160 1698\n",
      " 2439    1 2598   72   29  212  166    2  137  140    8 3144    5   27  125\n",
      "   81   37   24   17   28  531 4880   26   44 4999   53   14   32  281    2\n",
      "   90  157  486  415    4  495    7    7  446    2  156   10  856   10  261\n",
      " 3468 4999   14 4999 2373  172  133   26    6    8   26   44    1 4999  968\n",
      "  129  269    2  265 1366   42   11 4999  649   26   97 1668   24  202   17\n",
      "  205  147    7    7  587]\n"
     ]
    }
   ],
   "source": [
    "print train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at our input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approach\n",
    "\n",
    "Now that we already processed our data nice and clean, we are ready to build deep learning models. Before we build anything, probably the first idea is to ask *do we need to build our machine learning systems from scratch?* In most cases, the answer is NO, we can build our model based upon something (model architectures, model weights) which is already proven to work well and \"intuitively\" adapt to our specific problem.\n",
    "\n",
    "### Standing on the giants' shoulders\n",
    "\n",
    "Just like what we see in using convolutional networks with well-known architectures such as vgg or resnet and used their respective ImageNet pre-trained weights, we can leverage a lot of benefits such as our classification categories may well be contained in ImageNet 1000 categories, the networks already learned so many useful feaures, etc. Similarly, for NLP tasks like this we can ask is there anything similar to pretraind ImageNet weights? The answer is yes. If you think about it, pretrained words may be even clearer than pretrained images, why? because the word \"deep learning\" in any documents or test sets will be the same, it is \"deep learning\". Whereas pre-trained images of dogs may very different from the dogs in test images.\n",
    "\n",
    "By far the most popular \"pre-trained weights\" for words are Word2Vec from Google and GloVe from Stanford NLP lab. These are technically called word embeddings or word vector representations.\n",
    "\n",
    "### Represent stuff by numerical vectors\n",
    "\n",
    "Okie, but what is word embedding by the way?\n",
    "\n",
    "*** INSIGHT***\n",
    "\n",
    "Machine learning models work with numbers. Therefore, in order to input data (text, images, audio) to machine learning models we need somehow *convert* each of the data to a numeric value. Image and audio data are easy because they are intrinsically numerical in the way they are represented. How is about word?\n",
    "\n",
    "It is a good (primative) way to represent (or index) each word as an integer because it is easy to look up and manipulate. Fromt here, there are other methods to represent words: onehot encoding and word embedding (or word vectors):\n",
    "\n",
    "\n",
    "1. **Onehot encoding** explodes the whole vocabulary to \"onehot\" vectors. Each word is assigned to an vector which is of the length of vocabulary size. An onehot vector is zero everywhere except for where the entry is equal to the index of that word in the vocabulary. Onehot encoding is simple, intuitive and has many disadvantages:\n",
    "\n",
    " First, the size of input scales with the size of the vocab. If the vocab is 100000 words and each input sequence is of 10 words long, then one input is of the size 10x100000. Moreover, eventhough the input data is big, it is sparse, therefore, working with it is computaionally expensive. \n",
    " \n",
    " Secondly, the representation does not encode much useful semantic information. Due to the orthogonal and sparse representation, distance between any 2 word (norm-2) is exactly $\\sqrt2$. That means relationship between words \"dog\" and \"puppy\" is no difference from \"dog\" and \"engine\". \n",
    " \n",
    " How can improve onehot encoding? one idea is reduce the dimension of the representation by SVD. There are a lot of work done in this direction like latent semantic analysis but it also has it own problems such as computational expensive (SVD), issue with new words (have to run SVD every time a new word is introduced to the vocab) etc.\n",
    " \n",
    " Another direction is directly model every word by a dense and fixed length vector. That leads to the concept of word embeddings.\n",
    "\n",
    "\n",
    "- **Word embedding** maps each word with a numerical vector of prefixed length putting together to form a matrix. How to obtain these vectors? There are different ways, e.g., language models, to do this but the central idea is basically to characterize a word by the words around it (literally left and right).\n",
    "\n",
    "  \" You shall know a word by the company it keeps\" - J.R. Firth (1957)\n",
    " \n",
    " So, people (who trained word embeddings such as word2vec of Google or GloVe of Stanford NLP lab) trained their language model on large corpa such as wikipedia dump or entire internet dump. \n",
    " \n",
    " Specifically, for example, in Google word2vec's continuos bag of words model, the probability of any word in the corpus is conditioned to probabilities of, say, 10 words to the left and 10 words to the right by using the softmax function. Then the model is trained by maximizing the log-likehood using gradient descent for all the words in the entire corpus. There are some technical details on how to obtain the conditional probability by using negative sampling to avoid calculating the expensive total probablities. Nevertheless, the math is relatively simple.\n",
    " \n",
    " This training resutls in a vector representations of words that is able to capture semantic information such as in terms of their l2-distance: \"king\"-\"queen\" = \"man\"-\"woman\" or words like \"president\", \"minister\", \"governor\", \"senator\" etc are clustered closely to each other (politic cluster). Surprised? Magic? Not quite, if you think you about it. This is expected because these words normally appears in the same contexts, maybe same sentences. So natually as we maximize the probablity that they appear together we should certainly get these resutls.\n",
    "\n",
    "\n",
    "We will choose GloVe embedding for this particular problem because it is a litle bit faster than Google's one. It would be nice to use other word embeddings as well, to have a comparison, but we stick to GloVe for the moment. \n",
    "\n",
    "- We use GloVe 6 billion tokens downloaded from [here](https://nlp.stanford.edu/projects/glove/).\n",
    "- This version has 4 embeddings with different dimensions: 50, 100, 200, 300. First try we will use 50 as the dimension of our embedding. Then we can use others to compare.\n",
    "\n",
    "# 3. Preprocessing pretrained word embedding.\n",
    "\n",
    "After downloading the 6B token version from the link above, let's open the 50 dimensional embedding and split very line and look at the first line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(DATA_HOME_DIR +'glove.6B/glove.6B.50d.txt') as f: lines = [line.split() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe vocabulary size\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457', '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843', '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658', '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801', '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823', '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071', '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449', '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871', '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785', '0.0027849', '-0.18411', '-0.11514', '-0.78581']\n"
     ]
    }
   ],
   "source": [
    "print lines[0],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "so, the first element is the word 'the' and following it is 50 numerical values that represent that word.\n",
    "\n",
    "Now, let take the first element of every line and put into a list of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_vocab = [line[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n"
     ]
    }
   ],
   "source": [
    "# see first 20 words\n",
    "print glove_vocab[:20],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's index these words for quick lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordidx = {o:i for i,o in enumerate(glove_vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We also need to separate the numerical parts and put them into an 2darray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_embedding = np.stack(np.array([d[1:] for d in lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0.418' '0.24968' '-0.41242' '0.1217' '0.34527' '-0.044457' '-0.49688'\n",
      "  '-0.17862' '-0.00066023' '-0.6566' '0.27843' '-0.14767' '-0.55677'\n",
      "  '0.14658' '-0.0095095' '0.011658' '0.10204' '-0.12792' '-0.8443'\n",
      "  '-0.12181' '-0.016801' '-0.33279' '-0.1552' '-0.23131' '-0.19181'\n",
      "  '-1.8823' '-0.76746' '0.099051' '-0.42125' '-0.19526' '4.0071' '-0.18594'\n",
      "  '-0.52287' '-0.31681' '0.00059213' '0.0074449' '0.17778' '-0.15897'\n",
      "  '0.012041' '-0.054223' '-0.29871' '-0.15749' '-0.34758' '-0.045637'\n",
      "  '-0.44251' '0.18785' '0.0027849' '-0.18411' '-0.11514' '-0.78581']\n",
      " ['0.013441' '0.23682' '-0.16899' '0.40951' '0.63812' '0.47709' '-0.42852'\n",
      "  '-0.55641' '-0.364' '-0.23938' '0.13001' '-0.063734' '-0.39575'\n",
      "  '-0.48162' '0.23291' '0.090201' '-0.13324' '0.078639' '-0.41634'\n",
      "  '-0.15428' '0.10068' '0.48891' '0.31226' '-0.1252' '-0.037512' '-1.5179'\n",
      "  '0.12612' '-0.02442' '-0.042961' '-0.28351' '3.5416' '-0.11956'\n",
      "  '-0.014533' '-0.1499' '0.21864' '-0.33412' '-0.13872' '0.31806' '0.70358'\n",
      "  '0.44858' '-0.080262' '0.63003' '0.32111' '-0.46765' '0.22786' '0.36034'\n",
      "  '-0.37818' '-0.56657' '0.044691' '0.30392']]\n"
     ]
    }
   ],
   "source": [
    "print w_embedding[:2,:],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save these preprocessing data so that we do not have to run this again next time.\n",
    "\n",
    "### Save preprocessed GloVe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(glove_vocab, open(DATA_HOME_DIR+'glove.6B/glove_vocab_50.pkl','wb'))\n",
    "pickle.dump(wordidx, open(DATA_HOME_DIR+'glove.6B/word_idx_50.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data(path, array):\n",
    "    data=bcolz.carray(array, rootdir=path, mode='w')\n",
    "    data.flush()\n",
    "save_data(DATA_HOME_DIR+'glove.6B/w_embedding.dat', w_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Load  preprocessed GloVe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_embedding = bcolz.open(DATA_HOME_DIR+'glove.6B/w_embedding.dat')[:]\n",
    "glove_vocab= pickle.load(open(DATA_HOME_DIR+'glove.6B/glove_vocab_50.pkl','rb'))\n",
    "glove_idx= pickle.load(open(DATA_HOME_DIR+'glove.6B/word_idx_50.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish preprocessing GloVe word embedding data.\n",
    "\n",
    "# 4. Building Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
